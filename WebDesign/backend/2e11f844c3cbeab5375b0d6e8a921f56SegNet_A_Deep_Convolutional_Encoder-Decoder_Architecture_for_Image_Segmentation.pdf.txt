IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017 2481SegNet: A Deep Convolutional Encoder-DecoderArchitecture for Image SegmentationVijay Badrinarayanan, Alex Kendall , and Roberto Cipolla, Senior Member, IEEEAbstract—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentationtermed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followedby a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in theVGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution featuremaps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution inputfeature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder toperform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are thenconvolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2]and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versusaccuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understandingapplications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is alsosignificantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end usingstochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes andSUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance withcompetitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffeimplementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.Index Terms—Deep convolutional neural networks, semantic pixel-wise segmentation, indoor scenes, road scenes, encoder, decoder,pooling, upsamplingÇ1 INTRODUCTIONSEMANTIC segmentation has a wide array of applications Our architecture, SegNet, is designed to be an efficientranging from scene understanding, inferring support- architecture for pixel-wise semantic segmentation. It is pri-relationships among objects to autonomous driving. Early marily motivated by road scene understanding applicationsmethods that relied on low-level vision cues have fast been which require the ability to model appearance (road, build-superseded by popular machine learning algorithms. In par- ing), shape (cars, pedestrians) and understand the spatial-ticular, deep learning has seen huge success lately in hand- relationship (context) between different classes such as roadwritten digit recognition, speech, categorising whole images and side-walk. In typical road scenes, the majority of theand detecting objects in images [5]. Now there is an active pixels belong to large classes such as road, building andinterest for semantic pixel-wise labelling [2], [3], [4],[6], [7], hence the network must produce smooth segmentations.[8], [9], [10], [11], [12], [13], [14], [15]. However, some of these The engine must also have the ability to delineate objectsrecent approaches have tried to directly adopt deep architec- based on their shape despite their small size. Hence it istures designed for category prediction to pixel-wise labelling important to retain boundary information in the extracted[6]. The results, although very encouraging, appear coarse image representation. From a computational perspective, it[3]. This is primarily because max pooling and sub-sampling is necessary for the network to be efficient in terms of bothreduce feature map resolution. Our motivation to design memory and computation time during inference. The abilitySegNet arises from this need to map low resolution features to train end-to-end in order to jointly optimise all theto input resolution for pixel-wise classification. This map- weights in the network using an efficient weight updateping must produce features which are useful for accurate technique such as stochastic gradient descent (SGD) [16] isboundary localization. an additional benefit since it is more easily repeatable. Thedesign of SegNet arose from a need to match these criteria. The encoder network in SegNet is topologically identicalThe authors are with the Machine Intelligence Lab, Department ofEngineering, University of Cambridge, Cambridge CB2 1TN, United to the convolutional layers in VGG16 [1]. We remove theKingdom. E-mail: {vb292, agk34, cipolla}@eng.cam.ac.uk. fully connected layers of VGG16 which makes the SegNetManuscript received 7 Dec. 2015; revised 25 Aug. 2016; accepted 18 Dec. encoder network significantly smaller and easier to train2016. Date of publication 1 Jan. 2017; date of current version 10 Nov. 2017. than many other recent architectures [2], [4], [10], [17]. TheRecommended for acceptance by T. Brox. key component of SegNet is the decoder network whichFor information on obtaining reprints of this article, please send e-mail to:reprints@ieee.org, and reference the Digital Object Identifier below. consists of a hierarchy of decoders one corresponding toDigital Object Identifier no. 10.1109/TPAMI.2016.2644615 each encoder. Of these, the appropriate decoders use theThis work is licensed under a Creative Commons Attribution 3.0 License. For more information, see http://creativecommons.org/licenses/by/3.0/2482 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017Fig. 1. SegNet predictions on road scenes and indoor scenes. To try our system yourself, please see our online web demo at http://mi.eng.cam.ac.uk/projects/segnet/.max-pooling indices received from the corresponding decoding process used in some of these approaches [2], [4]encoder to perform non-linear upsampling of their input and reveal their pros and cons.feature maps. This idea was inspired from an architecture We evaluate the performance of SegNet on two scenedesigned for unsupervised feature learning [18]. Reusing segmentation tasks, CamVid road scene segmentation [21]max-pooling indices in the decoding process has several and SUN RGB-D indoor scene segmentation [22]. Pascalpractical advantages; (i) it improves boundary delineation, VOC12 [20] has been the benchmark challenge for segmen-(ii) it reduces the number of parameters enabling end-to- tation over the years. However, the majority of this task hasend training, and (iii) this form of upsampling can be incor- one or two foreground classes surrounded by a highly var-porated into any encoder-decoder architecture such as [2], ied background. This implicitly favours techniques used for[9] with only a little modification. detection as shown by the recent work on a decoupled clas-One of the main contributions of this paper is our analy- sification-segmentation network [17] where the classifica-sis of the SegNet decoding technique and the widely used tion network can be trained with a large set of weaklyFully Convolutional Network (FCN) [2]. This is in order to labelled data and the independent segmentation networkconvey the practical trade-offs involved in designing seg- performance is improved. The method of [3] also use thementation architectures. Most recent deep architectures for feature maps of the classification network with an indepen-segmentation have identical encoder networks, i.e, VGG16, dent CRF post-processing technique to perform segmenta-but differ in the form of the decoder network, training and tion. The performance can also be boosted by the useinference. Another common feature is they have trainable additional inference aids such as region proposals [4], [23].parameters in the order of hundreds of millions and thus Therefore, it is different from scene understanding whereencounter difficulties in performing end-to-end training [4]. the idea is to exploit co-occurrences of objects and other spa-The difficulty of training these networks has led to multi- tial-context to perform robust segmentation. To demon-stage training [2], appending networks to a pre-trained strate the efficacy of SegNet, we present a real-time onlinearchitecture such as FCN [9], use of supporting aids such as demo of road scene segmentation into 11 classes of interestregion proposals for inference [4], disjoint training of classi- for autonomous driving (see link in Fig. 1). Some examplefication and segmentation networks [17] and use of addi- test results produced on randomly sampled road scenetional training data for pre-training [10], [19] or for full images from Google and indoor test scenes from the SUNtraining [9]. In addition, performance boosting post-proc- RGB-D dataset [22] are shown in Fig. 1.essing techniques [3] have also been popular. Although all The remainder of the paper is organized as follows. Inthese factors improve performance on challenging bench- Section 2 we review related recent literature. We describemarks [20], it is unfortunately difficult from their quantita- the SegNet architecture and its analysis in Section 3. Intive results to disentangle the key design factors necessary Section 4 we evaluate the performance of SegNet on out-to achieve good performance. We therefore analysed the door and indoor scene datasets. This is followed by aBADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2483general discussion regarding our approach with pointers to are already an improvement over hand engineered featuresfuture work in Section 5. We conclude in Section 6. [6] but their ability to delineate boundaries is poor.Newer deep architectures [2], [4], [9], [12], [17] particu-2 LITERATURE REVIEW larly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution imageSemantic pixel-wise segmentation is an active topic of representations to pixel-wise predictions. The encoder net-research, fuelled by challenging datasets [20], [21], [22], [24], work which produces these low resolution representations[25]. Before the arrival of deep networks, the best performing in all of these architectures is the VGG16 classification net-methods mostly relied on hand engineered features classify- work [1] which has 13 convolutional layers and three fullying pixels independently. Typically, a patch is fed into a clas-connected layers. This encoder network weights are typi-sifier, e.g., Random Forest [26], [27] or Boosting [28], [29] tocally pre-trained on the large ImageNet object classificationpredict the class probabilities of the center pixel. Featuresdataset [40]. The decoder network varies between thesebased on appearance [26] or SfM and appearance [27], [28],architectures and is the part which is responsible for pro-[29] have been explored for the CamVid road scene under-ducing multi-dimensional features for each pixel forstanding test [21]. These per-pixel noisy predictions (oftenclassification.called unary terms) from the classifiers are then smoothed byusing a pair-wise or higher order CRF [28], [29] to improve Each decoder in the Fully Convolutional Network archi-the accuracy. More recent approaches have aimed to pro- tecture [2] learns to upsample its input feature map(s) andduce high quality unaries by trying to predict the labels for combines them with the corresponding encoder featureall the pixels in a patch as opposed to only the center pixel. map to produce the input to the next decoder. It is an archi-This improves the results of Random Forest based unaries tecture which has a large number of trainable parameters in[30] but thin structured classes are classified poorly. Dense the encoder network (134 M) but a very small decoder net-depth maps computed from the CamVid video have also work (0.5 M). The overall large size of this network makes itbeen used as input for classification using Random Forests hard to train end-to-end on a relevant task. Therefore, the[31]. Another approach argues for the use of a combination authors use a stage-wise training process. Here eachof popular hand designed features and spatio-temporal decoder in the decoder network is progressively added tosuper-pixelization to obtain higher accuracy [32]. The best an existing trained network. The network is grown until noperforming technique on the CamVid test [29] addresses the further increase in performance is observed. This growth isimbalance among label frequencies by combining object stopped after three decoders thus ignoring high resolutiondetection outputs with classifier predictions in a CRF frame- feature maps can certainly lead to loss of edge informationwork. The result of all these techniques indicate the need for [4]. Apart from training related issues, the need to reuse theimproved features for classification. encoder feature maps in the decoder makes it memoryIndoor RGBD pixel-wise semantic segmentation has also intensive in test time. We study this network in more detailgained popularity since the release of the NYU dataset [24]. as it the core of other recent architectures [9], [10].This dataset showed the usefulness of the depth channel to The predictive performance of FCN has been improvedimprove segmentation. Their approach used features such further by appending the FCN with a recurrent neural net-as RGB-SIFT, depth-SIFT and pixel location as input to a work (RNN) [9] and fine-tuning them on large datasets [20],neural network classifier to predict pixel unaries. The noisy [41]. The RNN layers mimic the sharp boundary delineationunaries are then smoothed using a CRF. Improvements capabilities of CRFs while exploiting the feature representa-were made using a richer feature set including LBP and tion power of FCN’s. They show a significant improvementregion segmentation to obtain higher accuracy [33] followed over FCN-8 but also show that this difference is reducedby a CRF. In more recent work [24], both class segmentation when more training data is used to train FCN-8. The mainand support relationships are inferred together using a com- advantage of the CRF-RNN is revealed when it is jointlybination of RGB and depth based cues. Another approach trained with an architecture such as the FCN-8. The fact thatfocuses on real-time joint reconstruction and semantic seg- joint training helps is also shown in other recent results [42],mentation, where Random Forests are used as the classifier [43]. Interestingly, the deconvolutional network [4] per-[34]. Gupta et al. [35] use boundary detection and hierarchi- forms significantly better than FCN although at the cost of acal grouping before performing category segmentation. more complex training and inference. This however raisesThe common attribute in all these approaches is the use of the question as to whether the perceived advantage of thehand engineered features for classification of either RGB or CRF-RNN would be reduced as the core feed-forward seg-RGBD images. mentation engine is made better. In any case, the CRF-RNNThe success of deep convolutional neural networks for network can be appended to any deep segmentation archi-object classification has more recently led researchers to tecture including SegNet.exploit their feature learning capabilities for structured pre- Multi-scale deep architectures are also being pursueddiction problems such as segmentation. There have also [12], [43]. They come in two flavours, (i) those which usebeen attempts to apply networks designed for object catego- input images at a few scales and corresponding deep fea-rization to segmentation, particularly by replicating the ture extraction networks, and (ii) those which combine fea-deepest layer features in blocks to match image dimensions ture maps from different layers of a single deep architecture[6], [36], [37], [38]. However, the resulting classification is [10], [44]. The common idea is to use features extracted atblocky [37]. Another approach using recurrent neural net- multiple scales to provide both local and global context [45]works [39] merges several low resolution predictions to cre- and the using feature maps of the early encoding layersate input image resolution predictions. These techniques retain more high frequency detail leading to sharper class2484 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017Fig. 2. An illustration of the SegNet architecture. There are no fully connected layers and hence it is only convolutional. A decoder upsamples its inputusing the transferred pool indices from its encoder to produce a sparse feature map(s). It then performs convolution with a trainable filter bank to den-sify the feature map. The final decoder output feature maps are fed to a soft-max classifier for pixel-wise classification.boundaries. Some of these architectures are difficult to train feature maps. For each sample, the indices of the max loca-due to their parameter size [12]. Thus a multi-stage training tions computed during pooling are stored and passed to theprocess is employed along with data augmentation. The decoder. The decoder upsamples the feature maps by usinginference is also expensive with multiple convolutional the stored pooled indices. It convolves this upsampled mappathways for feature extraction. Others [43] append a CRF using a trainable decoder filter bank to reconstruct the inputto their multi-scale network and jointly train them. How- image. This architecture was used for unsupervised pre-ever, these are not feed-forward at test time and require training for classification. A somewhat similar decodingoptimization to determine the MAP labels. technique is used for visualizing trained convolutional net-Several of the recently proposed deep architectures for works [46] for classification. The architecture of Ranzatosegmentation are not feed-forward in inference time [3], [4], et al. mainly focused on layer-wise feature learning using[17]. They require either MAP inference over a CRF [42], small input patches. This was extended by Kavukcuoglu[43] or aids such as region proposals [4] for inference. We et al. [47] to accept full image sizes as input to learn hierarchi-believe the perceived performance increase obtained by cal encoders. Both these approaches however did notusing a CRF is due to the lack of good decoding techniques attempt to use deep encoder-decoder networks for unsuper-in their core feed-forward segmentation engine. SegNet on vised feature training as they discarded the decoders afterthe other hand uses decoders to obtain features for accurate each encoder training. Here, SegNet differs from these archi-pixel-wise classification. tectures as the deep encoder-decoder network is trainedThe recently proposed Deconvolutional Network [4] and jointly for a supervised learning task and hence the decodersits semi-supervised variant the Decoupled network [17] use are an integral part of the network in test time.the max locations of the encoder feature maps (pooling indi- Other applications where pixel wise predictions areces) to perform non-linear upsampling in the decoder net- made using deep networks are image super-resolution [48]work. The authors of these architectures, independently of and depth map prediction from a single image [49]. TheSegNet (first submitted to CVPR 2015 [11]), proposed this authors in [49] discuss the need for learning to upsampleidea of decoding in the decoder network. However, their from low resolution feature maps which is the central topicencoder network consists of the fully connected layers from of this paper.the VGG-16 network which consists of about 90 percent ofthe parameters of their entire network. This makes trainingof their network very difficult and thus require additional 3 ARCHITECTUREaids such as the use of region proposals to enable training. SegNet has an encoder network and a correspondingMoreover, during inference these proposals are used and decoder network, followed by a final pixelwise classificationthis increases inference time significantly. From a bench- layer. This architecture is illustrated in Fig. 2. The encodermarking point of view, this also makes it difficult to evalu- network consists of 13 convolutional layers which corre-ate the performance of their architecture (encoder-decoder spond to the first 13 convolutional layers in the VGG16 net-network) without other aids. In this work we discard the work [1] designed for object classification. We can thereforefully connected layers of the VGG16 encoder network which initialize the training process from weights trained for clas-enables us to train the network using the relevant training sification on large datasets [40]. We can also discard theset using SGD optimization. Another recent method [3] fully connected layers in favour of retaining higher resolu-shows the benefit of reducing the number of parameters sig- tion feature maps at the deepest encoder output. This alsonificantly without sacrificing performance, reducing mem- reduces the number of parameters in the SegNet encoderory consumption and improving inference time. network significantly (from 134 to 14.7 M) as compared toOurworkwas inspired by the unsupervised feature learn- other recent architectures [2], [4] (see. Table 6). Each encodering architecture proposed by Ranzato et al. [18]. The key layer has a corresponding decoder layer and hence thelearningmodule is an encoder-decoder network. An encoder decoder network has 13 layers. The final decoder output isconsists of convolution with a filter bank, element-wise tanh fed to a multi-class soft-max classifier to produce class prob-non-linearity, max-pooling and sub-sampling to obtain the abilities for each pixel independently.BADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2485Fig. 3. An illustration of SegNet and FCN [2] decoders. a; b; c; d correspond to values in a feature map. SegNet uses the max pooling indices toupsample (without learning) the feature map(s) and convolves with a trainable decoder filter bank. FCN upsamples by learning to deconvolve theinput feature map and adds the corresponding encoder feature map to produce the decoder output. This feature map is the output of the max-poolinglayer (includes sub-sampling) in the corresponding encoder. Note that there are no trainable decoder filters in FCN.Each encoder in the encoder network performs convolu- their encoder inputs. The high dimensional feature repre-tion with a filter bank to produce a set of feature maps. sentation at the output of the final decoder is fed to a train-These are then batch normalized [50], [51]). Then an ele- able soft-max classifier. This soft-max classifies each pixelment-wise rectified-linear non-linearity (ReLU) maxð0; xÞ is independently. The output of the soft-max classifier is a Kapplied. Following that, max-pooling with a 2 2 window channel image of probabilities where K is the number ofand stride 2 (non-overlapping window) is performed and classes. The predicted segmentation corresponds to the classthe resulting output is sub-sampled by a factor of 2. Max- with maximum probability at each pixel.pooling is used to achieve translation invariance over small We add here that two other architectures, DeconvNetspatial shifts in the input image. Sub-sampling results in a [52] and U-Net [15] share a similar architecture to SegNetlarge input image context (spatial window) for each pixel in but with some differences. DeconvNet has a much largerthe feature map. While several layers of max-pooling and parameterization, needs more computational resources andsub-sampling can achieve more translation invariance for is harder to train end-to-end (Table 6), primarily due to therobust classification correspondingly there is a loss of spa- use of fully connected layers (albeit in a convolutional man-tial resolution of the feature maps. The increasingly lossy ner) We report several comparisons with DeconvNet later(boundary detail) image representation is not beneficial for in the paper Section 4.segmentation where boundary delineation is vital. There- As compared to SegNet, U-Net [15] (proposed for thefore, it is necessary to capture and store boundary informa- medical imaging community) does not reuse pooling indi-tion in the encoder feature maps before sub-sampling is ces but instead transfers the entire feature map (at the costperformed. If memory during inference is not constrained, of more memory) to the corresponding decoders and con-then all the encoder feature maps (after sub-sampling) can catenates them to upsampled (via deconvolution) decoderbe stored. This is usually not the case in practical applica- feature maps. There is no conv5 and max-pool five block intions and hence we propose a more efficient way to store U-Net as in the VGG net architecture. SegNet, on the otherthis information. It involves storing only the max-pooling hand, uses all of the pre-trained convolutional layer weightsindices, i.e, the locations of the maximum feature value in from VGG net as pre-trained weights.each pooling window is memorized for each encoder fea-ture map. In principle, this can be done using 2 bits for each 3.1 Decoder Variants2 2 pooling window and is thus much more efficient to Many segmentation architectures [2], [3], [4] share the samestore as compared to memorizing feature map(s) in float encoder network and they only vary in the form of theirprecision. As we show later in this work, this lower memory decoder network. Of these we choose to compare the Seg-storage results in a slight loss of accuracy but is still suitable Net decoding technique with the widely used Fully Convo-for practical applications. lutional Network decoding technique [2], [9].The appropriate decoder in the decoder network upsam- In order to analyse SegNet and compare its performanceples its input feature map(s) using the memorized max- with FCN (decoder variants) we use a smaller version ofpooling indices from the corresponding encoder feature SegNet, termed SegNet-Basic,1 which has four encoders andmap(s). This step produces sparse feature map(s). This Seg- four decoders. All the encoders in SegNet-Basic performNet decoding technique is illustrated in Fig. 3. These feature max-pooling and sub-sampling and the correspondingmaps are then convolved with a trainable decoder filter decoders upsample its input using the received max-pool-bank to produce dense feature maps. A batch normalization ing indices. Batch normalization is used after each convolu-step is then applied to each of these maps. Note that the tional layer in both the encoder and decoder network. Nodecoder corresponding to the first encoder (closest to the biases are used after convolutions and no ReLU non-linear-input image) produces a multi-channel feature map, ity is present in the decoder network. Further, a constantalthough its encoder input has three channels (RGB). This isunlike the other decoders in the network which produce 1. SegNet-Basic was earlier termed SegNet in a archival version offeature maps with the same number of size and channels as this paper [11].2486 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017kernel size of 7 7 over all the encoder and decoder layers variant of SegNet (SegNet-Basic-EncoderAddition). Here bothis chosen to provide a wide context for smooth labelling, the pooling indices for upsampling are used, followed by ai.e., a pixel in the deepest layer feature map (layer 4) can be convolution step to densify its sparse input. This is thentraced back to a context window in the input image of added element-wise to the corresponding encoder feature106 106 pixels. This small size of SegNet-Basic allows us maps to produce a decoders output.to explore many different variants (decoders) and train Another and more memory intensive FCN-Basic variantthem in reasonable time. Similarly we create FCN-Basic, a (FCN-Basic-NoDimReduction) is where there is no dimen-comparable version of FCN for our analysis which shares sionality reduction performed for the encoder feature maps.the same encoder network as SegNet-Basic but with the This implies that unlike FCN-Basic the final encoder featureFCN decoding technique (see Fig. 3) used in all its decoders. map is not compressed to K channels before passing it toOn the left in Fig. 3 is the decoding technique used by the decoder network. Therefore, the number of channels atSegNet (also SegNet-Basic), where there is no learning the end of each decoder is the same as the correspondinginvolved in the upsampling step. However, the upsampled encoder (i.e 64).maps are convolved with trainable multi-channel decoder We also tried other generic variants where feature mapsfilters to densify its sparse inputs. Each decoder filter has are simply upsampled by replication [6], or by using a fixedthe same number of channels as the number of upsampled (and sparse) array of indices for upsampling. These per-feature maps. A smaller variant is one where the decoder fil- formed quite poorly in comparison to the above variants. Aters are single channel, i.e they only convolve their corre- variant without max-pooling and sub-sampling in thesponding upsampled feature map. This variant (SegNet- encoder network (decoders are redundant) consumes moreBasic-SingleChannelDecoder) reduces the number of trainable memory, takes longer to converge and performs poorly.parameters and inference time significantly. Finally, please note that to encourage reproduction of ourOn the right in Fig. 3 is the FCN (also FCN-Basic) decod- results we release the Caffe implementation of all theing technique. The important design element of the FCN variants.2model is dimensionality reduction step of the encoder fea-ture maps. This compresses the encoder feature maps which 3.2 Trainingare then used in the corresponding decoders. Dimensional- We use the CamVid road scenes dataset to benchmark theity reduction of the encoder feature maps, say of 64 chan- performance of the decoder variants. This dataset is small,nels, is performed by convolving them with 1 1 64K consisting of 367 training and 233 testing RGB images (daytrainable filters, where K is the number of classes. The com- and dusk scenes) at 360 480 resolution. The challenge is topressed K channel final encoder layer feature maps are the segment 11 classes such as road, building, cars, pedestrians,input to the decoder network. In a decoder of this network, signs, poles, side-walk etc. We perform local contrast nor-upsampling is performed by inverse convolution using a malization [53] to the RGB input.fixed or trainable multi-channel upsampling kernel. We set the The encoder and decoder weights were all initializedkernel size to 8 8. This manner of upsampling is also using the technique described in He et al. [54]. To train alltermed as deconvolution. Note that, in comparison, SegNet the variants we use stochastic gradient descent with a fixedthe multi-channel convolution using trainable decoder fil- learning rate of 0.1 and momentum of 0.9 [16] using ourters is performed after upsampling to densifying feature Caffe implementation of SegNet-Basic [55]. We train themaps. The upsampled feature map in FCN has K channels. variants until the training loss converges. Before eachIt is then added element-wise to the corresponding resolu- epoch, the training set is shuffled and each mini-batch (12tion encoder feature map to produce the output decoder images) is then picked in order thus ensuring that eachfeature map. The upsampling kernels are initialized using image is used only once in an epoch. We select the modelbilinear interpolation weights [2]. which performs highest on a validation dataset.The FCN decoder model requires storing encoder fea- We use the cross-entropy loss [2] as the objective func-ture maps during inference. This can be memory intensive tion for training the network. The loss is summed up overfor embedded applications; for, e.g., storing 64 feature all the pixels in a mini-batch. When there is large variationmaps of the first layer of FCN-Basic at 180 240 resolution in the number of pixels in each class in the training setin 32 bit floating point precision takes 11 MB. This can be (e.g. road, sky and building pixels dominate the CamVidmade smaller using dimensionality reduction to the 11 fea- dataset) then there is a need to weight the loss differentlyture maps which requires  1.9 MB storage. SegNet on the based on the true class. This is termed class balancing. Weother hand requires almost negligible storage cost for the use median frequency balancing [12] where the weightpooling indices (.17 MB if stored using 2 bits per 2 2 assigned to a class in the loss function is the ratio of thepooling window). We can also create a variant of the FCN- median of class frequencies computed on the entire train-Basic model which discards the encoder feature map addi- ing set divided by the class frequency. This implies thattion step and only learns the upsampling kernels (FCN- larger classes in the training set have a weight smallerBasic-NoAddition). than 1 and the weights of the smallest classes are the high-In addition to the above variants, we study upsampling est. We also experimented with training the different var-using fixed bilinear interpolation weights which therefore iants without class balancing or equivalently using naturalrequires no learning for upsampling (Bilinear-Interpolation). frequency balancing.At the other extreme, we can add 64 encoder feature mapsat each layer to the corresponding output feature maps 2. See http://mi.eng.cam.ac.uk/projects/segnet/ for our SegNet codefrom the SegNet decoder to create a more memory intensive andweb demo.BADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2487TABLE 1Comparison of Decoder VariantsMedian frequency balancing Natural frequency balancingTest Train Test TrainStorage InferVariant Params (M) multiplier time (ms) G C mIoU BF G C mIoU G C mIoU BF G C mIoUFixed upsamplingBilinear-Interpolation 0.625 0 24.2 77.9 61.1 43.3 20.83 89.1 90.2 82.7 82.7 52.5 43.8 23.08 93.5 74.1 59.9Upsampling using max-pooling indicesSegNet-Basic 1.425 1 52.6 82.7 62.0 47.7 35.78 94.7 96. 2 92.7 84.0 54.6 46.3 36.67 96.1 83.9 73.3SegNet-Basic-EncoderAddition 1.425 64 53.0 83.4 63.6 48.5 35.92 94.3 95.8 92.0 84.2 56.5 47.7 36.27 95.3 80.9 68.9SegNet-Basic-SingleChannelDecoder 0.625 1 33.1 81.2 60.7 46.1 31.62 93.2 94.8 90.3 83.5 53.9 45.2 32.45 92.6 68.4 52.8Learning to upsample (bilinear initialisation)FCN-Basic 0.65 11 24.2 81.7 62.4 47.3 38.11 92.8 93.6 88.1 83.9 55.6 45.0 37.33 92.0 66.8 50.7FCN-Basic-NoAddition 0.65 n/a 23.8 80.5 58.6 44.1 31.96 92.5 93.0 87.2 82.3 53.9 44.2 29.43 93.1 72.8 57.6FCN-Basic-NoDimReduction 1.625 64 44.8 84.1 63.4 50.1 37.37 95.1 96.5 93.2 83.5 57.3 47.0 37.13 97.2 91.7 84.8FCN-Basic-NoAddition-NoDimReduction 1.625 0 43.9 80.5 61.6 45.9 30.47 92.5 94.6 89.9 83.7 54.8 45.5 33.17 95.0 80.2 67.8We quantify the performance using global (G), class average (C), mean of intersection over union (mIoU) and a semantic contour measure (BF). The testing andtraining accuracies are shown as percentages for both natural frequency and median frequency balanced training loss function. SegNet-Basic performs at thesame level as FCN-Basic but requires only storing max-pooling indices and is therefore more memory efficient during inference. Note that the theoretical memoryrequirement reported is based only on the size of the first layer encoder feature map. FCN-Basic, SegNet-Basic, SegNet-Basic-EncoderAddition all have high BFscores indicating the need to use information in encoder feature maps for better class contour delineation. Networks with larger decoders and those using theencoder feature maps in full perform best, although they are least efficient in terms of inference time and memory.3.3 Analysis We test each architectural variant after each 1,000 itera-To compare the quantitative performance of the different tions of optimization on the CamVid validation set until thedecoder variants, we use three commonly used perfor- training loss converges. With a training mini-batch size ofmance measures: global accuracy (G) which measures the 12 this corresponds to testing approximately every 33percentage of pixels correctly classified in the dataset, class epochs (passes) through the training set. We select the itera-average accuracy (C) is the mean of the predictive accuracy tion wherein the global accuracy is highest amongst theover all classes and mean intersection over union (mIoU) evaluations on the validation set. We report all the threeover all classes as used in the Pascal VOC12 challenge [20]. measures of performance at this point on the held-out Cam-The mIoU metric is a more stringent metric than class aver- Vid test set. Although we use class balancing while trainingage accuracy since it penalizes false positive predictions. the variants, it is still important to achieve high global accu-However, mIoU metric is not optimized for directly through racy to result in an overall smooth segmentation. Anotherthe class balanced cross-entropy loss. reason is that the contribution of segmentation towardsThe mIoU metric otherwise known as the Jacard Index is autonomous driving is mainly for delineating classes suchmost commonly used in benchmarking. However, Csurka as roads, buildings, side-walk, sky. These classes dominateet al. [56] note that this metric does not always correspond the majority of the pixels in an image and a high globalto human qualitative judgements (ranks) of good quality accuracy corresponds to good segmentation of these impor-segmentation. They show with examples that mIoU favours tant classes. We also observed that reporting the numericalregion smoothness and does not evaluate boundary accu- performance when class average is highest can often corre-racy, a point also alluded to recently by the authors of FCN spond to low global accuracy indicating a perceptually[57]. Hence they propose to complement the mIoU metric noisy segmentation output.with a boundary measure based on the Berkeley contour In Table 1 we report the numerical results of our analysis.matching score commonly used to evaluate unsupervised We also show the size of the trainable parameters and theimage segmentation quality [58]. Csurka et al. [56] simply highest resolution feature map or pooling indices storageextend this to semantic segmentation and show that the memory, i.e, of the first layer feature maps after max-pool-measure of semantic contour accuracy used in conjunction ing and sub-sampling. We show the average time for onewith the mIoU metric agrees more with human ranking of forward pass with our Caffe implementation, averagedsegmentation outputs. over 50 measurements using a 360 480 input on an NVI-The key idea in computing a semantic contour score is to DIA Titan GPU with cuDNN v3 acceleration. We note thatevaluate the F1-measure [58] which involves computing the the upsampling layers in the SegNet variants are not opti-precision and recall values between the predicted and mised using cuDNN acceleration. We show the results forground truth class boundary given a pixel tolerance dis- both testing and training for all the variants at the selectedtance. We used a value of 0.75 percent of the image diagonal iteration. The results are also tabulated without class balanc-as the tolerance distance. The F1-measure for each class that ing (natural frequency) for training and testing accuracies.is present in the ground truth test image is averaged to pro- Below we analyse the results with class balancing.duce an image F1-measure. Then we compute the whole From the Table 1, we see that bilinear interpolation basedtest set average, denoted the boundary F1-measure (BF) by upsampling without any learning performs the worst basedaverage the image F1 measures. on all the measures of accuracy. All the other methods2488 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017which either use learning for upsampling (FCN-Basic and best case, when both memory and inference time is not con-variants) or learning decoder filters after upsampling (Seg- strained, larger models such as FCN-Basic-NoDimReductionNet-Basic and its variants) perform significantly better. This and SegNet-EncoderAddition are both more accurate thanemphasizes the need to learn decoders for segmentation. the other variants. Particularly, discarding dimensionalityThis is also supported by experimental evidence gathered reduction in the FCN-Basic model leads to the best perfor-by other authors when comparing FCN with SegNet-type mance amongst the FCN-Basic variants with a high BF score.decoding techniques [4]. This once again emphasizes the trade-off involved betweenWhen we compare SegNet-Basic and FCN-Basic we see memory and accuracy in segmentation architectures.that both perform equally well on this test over all the meas- The last two columns of Table 1 show the result when noures of accuracy. The difference is that SegNet uses less class balancing is used (natural frequency). Here, we canmemory during inference since it only stores max-pooling observe that without weighting the results are poorer for allindices. On the other hand FCN-Basic stores encoder feature the variants, particularly for class average accuracy andmaps in full which consumes much more memory (11 times mIoU metric. The global accuracy is the highest withoutmore). SegNet-Basic has a decoder with 64 feature maps in weighting since the majority of the scene is dominated byeach decoder layer. In comparison FCN-Basic, which uses sky, road and building pixels. Apart from this all the infer-dimensionality reduction, has fewer (11) feature maps in ence from the comparative analysis of variants holds trueeach decoder layer. This reduces the number of convolu- for natural frequency balancing too, including the trends fortions in the decoder network and hence FCN-Basic is faster the BF measure. SegNet-Basic performs as well as FCN-during inference (forward pass). From another perspective, Basic and is better than the larger FCN-Basic-NoAddition-the decoder network in SegNet-Basic makes it overall a NoDimReduction. The bigger but less efficient modelslarger network than FCN-Basic. This endows it with more FCN-Basic-NoDimReduction and SegNet-EncoderAdditionflexibility and hence achieves higher training accuracy than perform better than the other variants.FCN-Basic for the same number of iterations. Overall we We can now summarize the above analysis with the fol-see that SegNet-Basic has an advantage over FCN-Basic lowing general points.when inference time memory is constrained but whereinference time can be compromised to some extent. 1) The best performance is achieved when encoder fea-SegNet-Basic is most similar to FCN-Basic-NoAddition ture maps are stored in full. This is reflected in thein terms of their decoders, although the decoder of SegNet semantic contour delineationmetric (BF)most clearly.is larger. Both learn to produce dense feature maps, either 2) When memory during inference is constrained,directly by learning to perform deconvolution as in FCN- then compressed forms of encoder feature mapsBasic-NoAddition or by first upsampling and then convolv- (dimensionality reduction, max-pooling indices) caning with trained decoder filters. The performance of Seg- be stored and used with an appropriate decoderNet-Basic is superior, in part due to its larger decoder size. (e.g., SegNet type) to improve performance.The accuracy of FCN-Basic-NoAddition is also lower as 3) Larger decoders increase performance for a givencompared to FCN-Basic. This shows that it is vital to capture encoder network.the information present in the encoder feature maps for bet-ter performance. In particular, note the large drop in the BF 4 BENCHMARKINGmeasure between these two variants. This can also explain Wequantify the performance of SegNet on two scene segmen-the part of the reason why SegNet-Basic outperforms FCN- tation benchmarks using our Caffe implementation.3 The firstBasic-NoAddition. task is road scene segmentation which is of current practicalThe size of the FCN-Basic-NoAddition-NoDimReduction interest for various autonomous driving related problems.model is slightly larger than SegNet-Basic since the final The second task is indoor scene segmentation which is ofencoder feature maps are not compressed to match the immediate interest to several augmented reality (AR) applica-number of classes K. This makes it a fair comparison in tions. The input RGB images for both taskswere 360 480.terms of the size of the model. The performance of this FCN We benchmarked SegNet against several other wellvariant is poorer than SegNet-Basic in test but also its train- adopted deep architectures for segmentation such as FCNing accuracy is lower for the same number of training [2], DeepLab-LargFOV [3] and DeconvNet [4]. Our objectiveepochs. This shows that using a larger decoder is not was to understand the performance of these architecturesenough but it is also important to capture encoder feature when trained end-to-end on the same datasets. To enablemap information to learn better, particular the fine grained end-to-end training we added batch normalization [50]contour information (notice the drop in the BF measure). layers after each convolutional layer. For DeepLab-Large-Here it is also interesting to see that SegNet-Basic has a com- FOV, we changed the max pooling 3 stride to 1 to achieve apetitive training accuracy when compared to larger models final predictive resolution of 45 60. We restricted the fea-such FCN-Basic-NoDimReduction. ture size in the fully connnected layers of DeconvNet toAnother interesting comparison between FCN-Basic- 1,024 so as to enable training with the same batch size asNoAddition and SegNet-Basic-SingleChannelDecoder shows other models. Here note that the authors of DeepLab-that usingmax-pooling indices for upsampling and an overall LargeFOV [3] have also reported little loss in performancelarger decoder leads to better performance. This also lends by reducing the size of the fully connected layers.evidence to SegNet being a good architecture for segmenta-tion, particularly when there is a need to find a compromise 3. Our web demo and Caffe implementation is available for evalua-between storage cost, accuracy versus inference time. In the tion at http://mi.eng.cam.ac.uk/projects/segnet/BADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2489In order to perform a controlled benchmark we used the to the rest of the methods. It is able to segment both smallsame SGD solver [16] with a fixed learning rate of 103 and and large classes well. We remark here that we used medianmomentum of 0.9. The optimization was performed for frequency class balancing [49] in training SegNet-Basic andmore than 100 epochs through the dataset until no further SegNet. In addition, there is an overall smooth quality ofperformance increase was observed. Dropout of 0.5 was segmentation much like what is typically obtained withadded to the end of deeper convolutional layers in all mod- CRF post-processing. Although the fact that results improveels to prevent overfitting (see http://mi.eng.cam.ac.uk/ with larger training sets is not surprising, the percentageprojects/segnet/tutorial.html for example caffe prototxt). improvement obtained using pre-trained encoder networkFor the road scenes which have 11 classes we used a mini- and this training set indicates that this architecture canbatch size of 5 and for indoor scenes with 37 classes we potentially be deployed for practical applications. Our ran-used a mini-batch size of 4. dom testing on urban and highway images from the internet(see Fig. 1) demonstrates that SegNet can absorb a large4.1 Road Scene Segmentation training set and generalize well to unseen images. It alsoA number of road scene datasets are available for semantic indicates the contribution of the prior (CRF) can be lessenedparsing [21], [25], [59], [60]. Of these we choose to bench- when sufficient amount of training data is made available.mark SegNet using the CamVid dataset [21] as it contains In Table 3 we compare SegNet’s performance with nowvideo sequences. This enables us to compare our proposed widely adopted fully convolutional architectures for seg-architecture with those which use motion and structure [27], mentation. As compared to the experiment in Table 2, we[28], [29] and video segments [32]. We also combine [21], did not use any class blancing for training any of the deep[25], [59], [60] to form an ensemble of 3433 images to train architectures including SegNet. This is because we found itSegNet for an additional benchmark. For a web demo (see difficult to train larger models such as DeconvNet withfootnote 3) of road scene segmentation, we include the Cam- median frequency balancing. We benchmark performanceVid test set to this larger dataset. Here, we would like to at 40 K, 80 K and > 80 K iterations which given the mini-note that another recent and independent segmentation batch size and training set size approximately correspondsbenchmark on road scenes has been performed for SegNet to 50, 100 and > 100 epochs. For the last test point we alsoand the other competing architectures used in this paper report the maximum number of iterations (here atleast 150[61]. However, the benchmark was not controlled, meaning epochs) beyond which we observed no accuracy improve-that each architecture was trained with a separate recipe ments or when over-fitting set in. We report the metrics atwith varying input resolutions and sometimes with a vali- three stages in the training phase to reveal how the metricsdation set included. Therefore, we believe our more con- varied with training time, particularly for larger networks.trolled benchmark can be used to complement their efforts. This is important to understand if additional trainingThe qualitative comparisons of SegNet predictions with time is justified when set against accuracy increases. Noteother deep architectures can be seen in Fig. 4. The qualita- also that for each evaluation we performed a complete runtive results show the ability of the proposed architecture to through the dataset to obtain batch norm statistics andsegment smaller classes in road scenes while producing a then evaluated the test model with this statistic (see http://smooth segmentation of the overall scene. Indeed, under mi.eng.cam.ac.uk/projects/segnet/tutorial.html for code.).the controlled benchmark setting, SegNet shows superior These evaluations are expensive to perform on large train-performance as compared to some of the larger models. ing sets and hence we only report metrics at three timeDeepLab-LargeFOV is the most efficient model and with points in the training phase.CRF post-processing can produce competitive results From Table 3 we immediately see that SegNet, Deconv-although smaller classes are lost. FCN with learnt deconvo- Net achieve the highest scores in all the metrics as com-lution is clearly better than with fixed bilinear upsampling. pared to other models. DeconvNet has a higher boundaryDeconvNet is the largest model and the most inefficient to delineation accuracy but SegNet is much more efficient astrain. Its predictions do not retain small classes. compared to DeconvNet. This can be seen from the computeWe also use this benchmark to first compare SegNet with statistics in Table 6. FCN, DeconvNet which have fully con-several non deep-learning methods including Random For- nected layers (turned into convolutional layers) train muchests [26], Boosting [26], [28] in combination with CRF based more slowly and have comparable or higher forward-back-methods [29]. This was done to give the user a perspective ward pass time with reference to SegNet. Here we note alsoof the improvements in accuracy that has been achieved that over-fitting was not an issue in training these largerusing deep networks compared to classical feature engi- models, since at comparable iterations to SegNet their met-neering based techniques. rics showed an increasing trend.The results in Table 2 show SegNet-Basic, SegNet obtain For the FCN model learning the deconvolutional layerscompetitive results when compared with methods which as opposed to fixing them with bi-linear interpolationuse CRFs. This shows the ability of the deep architecture to weights improves performance particularly the BF score. Itextract meaningful features from the input image and map also achieves higher metrics in a far lesser time. This factit to accurate and smooth class segment labels. The most agrees with our earlier analysis in Section 3.3.interesting result here is the large performance improve- Surprisingly, DeepLab-LargeFOV which is trained toment in class average and mIOU metrics that is obtained predict labels at a resolution of 45 60 produces competi-when a large training dataset, obtained by combining [21], tive performance given that it is the smallest model in terms[25], [59], [60], is used to train SegNet. Correspondingly, the of parameterization and also has the fastest training time asqualitative results of SegNet (see Fig. 4) are clearly superior per Table 6. However, the boundary accuracy is poorer and2490 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017Fig. 4. Results on CamVid day and dusk test samples. SegNet shows superior performance, particularly with its ability to delineate boundaries, ascompared to some of the larger models when all are trained in a controlled setting. DeepLab-LargeFOV is the most efficient model and with CRFpost-processing can produce competitive results although smaller classes are lost. FCN with learnt deconvolution is clearly better. DeconvNet is thelargest model with the longest training time, but its predictions loose small classes. Note that these results correspond to the model corresponding tothe highest mIoU accuracy in Table 3.this is shared by the other architectures. DeconvNet’s BF process on a subset of the training set since no validationscore is higher than the other networks when trained for a set was available.very long time. Given our analysis in Section 3.3 and thefact that it shares a SegNet type architecture. 4.2 SUN RGB-D Indoor ScenesThe impact of dense CRF [62] post-processing can be SUN RGB-D [22] is a very challenging and large dataset ofseen in the last time point for DeepLab-LargeFOV-den- indoor scenes with 5,285 training and 5,050 testing images.seCRF. Both global and mIoU improve but class average The images are captured by different sensors and hencediminshes. However a large improvement is obtained for come in various resolutions. The task is to segment 37the BF score. Note here that the dense CRF hyperpara- indoor scene classes including wall, floor, ceiling, table,meters were obtained by an expensive grid-search chair, sofa etc. This task is made hard by the fact that objectBADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2491TABLE 2Quantitative Comparisons of SegNet with Traditional Methods on the CamVid 11 Road Class Segmentation Problem [22]MethodSfM+Appearance [28] 46.2 61.9 89.7 68.6 42.9 89.5 53.6 46.6 0.7 60.5 22.5 53.0 69.1 n/aBoosting [29] 61.9 67.3 91.1 71.1 58.5 92.9 49.5 37.6 25.8 77.8 24.7 59.8 76.4 n/aDense Depth Maps [32] 85.3 57.3 95.4 69.2 46.5 98.5 23.8 44.3 22.0 38.1 28.7 55.4 82.1 n/aStructured Random Forests [31] n/a 51.4 72.5 n/aNeural Decision Forests [64] n/a 56.1 82.1 n/aLocal Label Descriptors [65] 80.7 61.5 88.8 16.4 n/a 98.0 1.09 0.05 4.13 12.4 0.07 36.3 73.6 n/aSuper Parsing [33] 87.0 67.1 96.9 62.7 30.1 95.9 14.7 17.9 1.7 70.0 19.4 51.2 83.3 n/aSegNet (3.5K dataset training - 140K) 89.6 83.4 96.1 87.7 52.7 96.4 62.2 53.45 32.1 93.3 36.5 71.20 90.40 60.10 46.84CRF based approachesBoosting + pairwise CRF [29] 70.7 70.8 94.7 74.4 55.9 94.1 45.7 37.2 13.0 79.3 23.1 59.9 79.8 n/aBoosting+Higher order [29] 84.5 72.6 97.5 72.7 34.1 95.3 34.2 45.7 8.1 77.6 28.5 59.2 83.8 n/aBoosting+Detectors+CRF [30] 81.5 76.6 96.2 78.7 40.2 93.9 43.0 47.6 14.3 81.5 33.9 62.5 83.8 n/aSegNet outperforms all the other methods, including those using depth, video and/or CRF’s on the majority of classes. In comparison with the CRF based methodsSegNet predictions are more accurate in 8 out of the 11 classes. It also shows a good  10 percent improvement in class average accuracy when trained on a largedataset of 3.5 K images. Particularly noteworthy are the significant improvements in accuracy for the smaller/thinner classes. * Note that we could not access pre-dictions for older methods for computing the mIoU, BF metrics.classes come in various shapes, sizes and in different poses. points can vary a lot and there is less regularity in both theThere are frequent partial occlusions since there are typi- number of classes present in a scene and their spatialcally many different classes present in each of the test arrangement. Another difficulty is caused by the widelyimages. These factors make this one of the hardest segmen- varying sizes of the object classes in the scene. Some testtation challenges. We only use the RGB modality for our samples from the recent SUN RGB-D dataset [22] are showntraining and testing. Using the depth modality would neces- in Fig. 5. We observe some scenes with few large classessitate architectural modifications/redesign [2]. Also the and some others with dense clutter (bottom row and right).quality of depth images from current cameras require care- The appearance (texture and shape) can also widely vary inful post-processing to fill-in missing measurements. They indoor scenes. Therefore, we believe this is the hardest chal-may also require using fusion of many frames to robustly lenge for segmentation architectures and methods in com-extract features for segmentation. Therefore we believe puter vision. Other challenges, such as Pascal VOC12 [20]using depth for segmentation merits a separate body of salient object segmentation have occupied researchers morework which is not in the scope of this paper. We also note [65], but we believe indoor scene segmentation is more chal-that an earlier benchmark dataset NYUv2 [24] is included as lenging and has more current practical applications such aspart of this dataset. in AR and robotics. To encourage more research in thisRoad scene images have limited variation, both in terms direction we compared well known deep architectures onof the classes of interest and their spatial arrangements. the large SUN RGB-D dataset.When captured from a moving vehicle where the camera The qualitative results of SegNet on samples of indoorposition is nearly always parallel to the road surface limit- scenes of different types such as bedroom, living room, labo-ing variability in view points. This makes it easier for deep ratory, meeting room, bathroom are shown in Fig. 5. We seenetworks to learn to segment them robustly. In comparison, that SegNet obtains reasonable predictions when the size ofimages of indoor scenes are more complex since the view the classes are large under different view points. This isTABLE 3Quantitative Comparison of Deep Networks for Semantic Segmentation on the CamVid Test Set When Trainedon a Corpus of 3,433 Road ScenesWithout Class BalancingNetwork/Iterations 40 K 80 K > 80 K Max iterG C mIoU BF G C mIoU BF G C mIoU BFSegNet 88.81 59.93 50.02 35.78 89.68 69.82 57.18 42.08 90.40 71.20 60.10 46.84 140 KDeepLab-LargeFOV[3] 85.95 60.41 50.18 26.25 87.76 62.57 53.34 32.04 88.20 62.53 53.88 32.77 140 KDeepLab-LargeFOV-denseCRF[3] not computed 89.71 60.67 54.74 40.79 140 KFCN 81.97 54.38 46.59 22.86 82.71 56.22 47.95 24.76 83.27 59.56 49.83 27.99 200 KFCN (learnt deconv) [2] 83.21 56.05 48.68 27.40 83.71 59.64 50.80 31.01 83.14 64.21 51.96 33.18 160 KDeconvNet [4] 85.26 46.40 39.69 27.36 85.19 54.08 43.74 29.33 89.58 70.24 59.77 52.23 260 KWhen end-to-end training is performed with the same and fixed learning rate, smaller networks like SegNet learn to perform better in a shorter time. The BF scorewhich measures the accuracy of inter-class boundary delineation is significantly higher for SegNet, DeconvNet as compared to other competing models. Deconv-Net matches the metrics for SegNet but at a much larger computational cost. Also see Table 2 for individual class accuracies for SegNet.BuildingTreeSkyCarSign-SymbolRoadPedestrianFenceColumn-PoleSide-walkBicyclistClass avg.Global avg.mIoUBF2492 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017Fig. 5. Qualitative assessment of SegNet predictions on RGB indoor test scenes from the recently released SUN RGB-D dataset [23]. In this hardchallenge, SegNet predictions delineate inter class boundaries well for object classes in a variety of scenes and their view-points. Overall rhe seg-mentation quality is better when object classes are reasonably sized but is very noisy when the scene is more cluttered. Note that often parts of animage of a scene do not have ground truth labels and these are shown in black colour. These parts are not masked in the corresponding deep modelpredictions that are shown. Note that these results correspond to the model corresponding to the highest mIoU accuracy in Table 4.particularly interesting since the input modality is only RGB. DeepLab-LargeFOV. As a stand alone experiment weRGB images are also useful to segment thinner structures trained SegNet with median frequency class balancing [66]such as the legs of chairs and tables, lamps which is difficult and the metrics were higher (see Table 4) and this agreesto achieve using depth images from currently available sen- with our analysis in Section 3.3. Interestingly, using the gridsors. This can be seen from the results of SegNet, DeconvNet search based optimal hyperparameters for the dense-CRFin Fig. 5. It is also useful to segment decorative objects such worsened all except the BF score metric for DeepLab-Large-as paintings on the wall for AR tasks. However as compared FOV-denseCRF. More optimal settings could perhaps beto outdoor scenes the segmentation quality is clearly more found but the grid search process was too expensive givennoisy. The quality drops significantly when clutter is the large inference time for dense-CRFs.increased (see the result sample in themiddle column). One reason for the overall poor performance is the largeThe quantitative results in Table 4 show that all the deep number of classes in this segmentation task, many of whicharchitectures share low mIoU and boundary metrics. The occupy a small part of the image and appear infrequently.global and class averages (correlates well with mIou) are The accuracies reported in Table 5 clearly show that largeralso small. SegNet outperforms all other methods in terms classes have reasonable accuracy and smaller classes haveof G,C, BF metrics and has a slightly lower mIoU than lower accuracies. This can be improved with larger sizedBADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2493TABLE 4Quantitative Comparison of Deep Architectures on the SUNRGB-D Dataset When Trained on a Corpus of 5,250 Indoor ScenesNetwork/Iterations 80 K 140 K > 140 K Max iterG C mIoU BF G C mIoU BF G C mIoU BFSegNet 70.73 30.82 22.52 9.16 71.66 37.60 27.46 11.33 72.63 44.76 31.84 12.66 240 KDeepLab-LargeFOV [3] 70.70 41.75 30.67 7.28 71.16 42.71 31.29 7.57 71.90 42.21 32.08 8.26 240 KDeepLab-LargeFOV-denseCRF [3] not computed 66.96 33.06 24.13 9.41 240 KFCN (learnt deconv) [2] 67.31 34.32 24.05 7.88 68.04 37.2 26.33 9.0 68.18 38.41 27.39 9.68 200 KDeconvNet [4] 59.62 12.93 8.35 6.50 63.28 22.53 15.14 7.86 66.13 32.28 22.57 10.47 380 KNote that only the RGB modality was used in these experiments. In this complex task with 37 classes all the architectures perform poorly, particularly because ofthe smaller sized classes and skew in the class distribution. DeepLab-Large FOV, the smallest and most efficient model has a slightly higher mIoU but SegNet hasa better G,C,BF score. Also note that when SegNet was trained with median frequency class balancing it obtained 71.75, 44.85, 32.08, 14.06 (180 K) as themetrics.TABLE 5Class Average Accuracies of SegNet Predictions for the 37 Indoor Scene Classes in the SUN RGB-D Benchmark DatasetWall Floor Cabinet Bed Chair Sofa Table Door Window Bookshelf Picture Counter Blinds83.42 93.43 63.37 73.18 75.92 59.57 64.18 52.50 57.51 42.05 56.17 37.66 40.29Desk Shelves Curtain Dresser Pillow Mirror Floor mat Clothes Ceiling Books Fridge TV Paper11.92 11.45 66.56 52.73 43.80 26.30 0.00 34.31 74.11 53.77 29.85 33.76 22.73Towel Shower curtain Box Whiteboard Person Night stand Toilet Sink Lamp Bathtub Bag19.83 0.03 23.14 60.25 27.27 29.88 76.00 58.10 35.27 48.86 16.76The performance correlates well with size of the classes in indoor scenes. Note that class average accuracy has a strong correlation with mIoU metric.TABLE 6A Comparison of Computational Time and Hardware Resources Required for Various Deep ArchitecturesNetwork Forward Backward GPU training GPU inference Modelpass(ms) pass(ms) memory (MB) memory (MB) size (MB)SegNet 422.50 488.71 6803 1,052 117DeepLab-LargeFOV [3] 110.06 160.73 5618 1,993 83FCN (learnt deconv) [2] 317.09 484.11 9735 1,806 539DeconvNet [4] 474.65 602.15 9731 1,872 877The caffe time command was used to compute time requirement averaged over 10 iterations with mini batch size 1 and an image of 360 480 resolu-tion We used nvidia-smi unix command to compute memory consumption. For training memory computation we used a mini-batch of size 4 and forinference memory the batch size was 1. Model size was the size of the caffe models on disk. SegNet is most memory efficient during inference model.datasets and class distribution aware training techniques. time memory and computational load are important toAnother reason for poor performance could lie in the inabil- deploy models on specialised embedded devices, for exam-ity of these deep architectures (all are based on the VGG ple, in AR applications. From an overall efficiency view-architecture) to large variability in indoor scenes. This con- point, we feel less attention has been paid to smaller andjecture on our part is based on the fact that the smallest more memory, time efficient models for real-time applica-model DeepLab-LargeFOV produces the best accuracy in tions such as road scene understanding and AR. This wasterms of mIoU and in comparison, larger parameterizations the primary motivation behind the proposal of SegNet,in DeconvNet, FCN did not improve perfomance even with which is significantly smaller and faster than other compet-much longer training (DeconvNet). This suggests there ing architectures, but which we have shown to be efficientcould lie a common reason for poor performance across all for tasks such as road scene understanding.architectures. More controlled datasets [67] are needed to Segmentation challenges such as Pascal [20] and MS-verify this hypothesis. COCO [41] are object segmentation challenges wherein afew classes are present in any test image. Scene segmenta-tion is more challenging due to the high variability of indoor5 DISCUSSION AND FUTURE WORK scenes and a need to segment a larger number of classesDeep learning models have often achieved increasing suc- simultaneously. The task of outdoor and indoor scene seg-cess due to the availability of massive datasets and expand- mentation are also more practically oriented with currenting model depth and parameterisation. However, in applications such as autonomous driving, robotics and AR.practice factors like memory and computational time dur- The metrics we chose to benchmark various deep seg-ing training and testing are important factors to consider mentation architectures like the boundary F1-measure (BF)when choosing a model from a large bank of models. Train- was done to complement the existing metrics which areing time becomes an important consideration particularly more biased towards region accuracies. It is clear from ourwhen the performance gain is not commensurate with experiments and other independent benchmarks [61] thatincreased training time as shown in our experiments. Test outdoor scene images captured from a moving car are easier2494 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 12, DECEMBER 2017to segment and deep architectures perform robustly. We [5] C. Szegedy, et al., “Going deeper with convolutions,” in Proc.IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 1–9.hope our experiments will encourage researchers to engage [6] C. Farabet, C. Couprie, L. Najman, and Y. LeCun, “Learning hier-their attention towards the more challenging indoor scene archical features for scene labeling,” IEEE Trans. Pattern Anal.segmentation task. Mach. Intell., vol. 35, no. 8, pp. 1915–1929, Aug. 2013.An important choice we had to make when benchmark- [7] N. Hft, H. Schulz, and S. Behnke, “Fast semantic segmentation ofRGB-D scenes with GPU-accelerated deep neural networks,” ining different deep architectures of varying parameterization Proc. 37th German Conf. Advances Artif. Intell., 2014, pp. 80–85.was the manner in which to train them. Many of these archi- [8] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng, “Parsing naturaltectures have used a host of supporting techniques and scenes and natural language with recursive neural networks,” inmulti-stage training recipes to arrive at high accuracies on Proc. 26th Int. Conf. Mach. Learn., 2011, pp. 129–136.[9] S. Zheng, et al., “Conditional random fields as recurrent neuraldatasets but this makes it difficult to gather evidence about networks,” inProc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1529–1537.their true performance under time and memory constraints. [10] W. Liu, A. Rabinovich, and A. C. Berg, “ParseNet: Looking widerInstead we chose to perform a controlled benchmarking to see better,” arXiv preprint arXiv:1506.04579, 2015.[11] V. Badrinarayanan, A. Handa, and R. Cipolla, “SegNet: A deepwhere we used batch normalization to enable end-to-end convolutional encoder-decoder architecture for robust semantictraining with the same solver (SGD). However, we note that pixel-wise labelling,” arXiv preprint arXiv:1505.07293, 2015.this approach cannot entirely disentangle the effects of [12] D. Eigen and R. Fergus, “Predicting depth, surface normals andmodel versus solver (optimization) in achieving a particular semantic labels with a common multi-scale convolutional archit-ecture,” in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 2650–2658.result. This is mainly due to the fact that training these net- [13] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille,works involves gradient back-propagation which is imper- “Weakly-and semi-supervised learning of a DCNN for semanticfect and the optimization is a non-convex problem in image segmentation,” arXiv:1502.02734, 2015.[14] F. Yu and V. Koltun, “Multi-scale context aggregation by dilatedextremely large dimensions. Acknowledging these short-convolutions,” arXiv:1511.07122, 2015.comings, our hope is that this controlled analysis comple- [15] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutionalments other benchmarks [61] and reveals the practical networks for biomedical image segmentation,” in Proc. Med. Imagetrade-offs involved in different well known architectures. Comput. Comput.-Assisted Intervention, 2015, pp. 234–241.[16] L. Bottou, “Large-scale machine learning with stochastic gradientFor the future, we would like to exploit our understand- descent,” in Proc. 19th Int. Conf. Comput. Statist., 2010, pp. 177–186.ing of segmentation architectures gathered from our analy- [17] S. Hong, H. Noh, and B. Han, “Decoupled deep neural networksis to design more efficient architectures for real-time for semi-supervised semantic segmentation,” in Proc. 28th Int.applications. We are also interested in estimating the model Conf. Neural Inf. Process. Syst., 2015, pp. 1495–1503.[18] M. Ranzato, F. J. Huang, Y. Boureau, and Y. LeCun,uncertainty for predictions from deep segmentation archi- “Unsupervised learning of invariant feature hierarchies withtectures [68], [69]. applications to object recognition,” in Proc. IEEE Conf. Comput.Vis. Pattern Recognit., 2007, pp. 1–8.6 C [19] R. Mottaghi, et al., “The role of context for object detection andONCLUSION semantic segmentation in the wild,” in Proc. IEEE Conf. Comput.We presented SegNet, a deep convolutional network architec- Vis. Pattern Recognit., 2014, pp. 891–898.[20] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,ture for semantic segmentation. The main motivation behind and A. Zisserman, “The pascal visual object classes challenge: ASegNet was the need to design an efficient architecture for retrospective,” Int. J. Comput. Vis., vol. 111, no. 1, pp. 98–136, 2015.road and indoor scene understanding which is efficient both [21] G. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classesin terms of memory and computational time. We analysed in video: A high-definition ground truth database,” Pattern Recog-nit. Lett., vol. 30, no. 2, pp. 88–97, 2009.SegNet and compared it with other important variants to [22] S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-Dreveal the practical trade-offs involved in designing architec- scene understanding benchmark suite,” in Proc. IEEE Conf. Com-tures for segmentation, particularly training time, memory put. Vis. Pattern Recognit., 2015, pp. 567–576.[23] C. L. Zitnick and P. Dollar, “Edge boxes: Locating object proposalsversus accuracy. Those architectures which store the encoderfrom edges,” in Proc. 13th Eur. Conf. Comput. Vis., 2014, pp. 391–405.network feature maps in full perform best but consume more [24] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmen-memory during inference time. SegNet on the other hand is tation and support inference from RGBD images,” in Proc. 12thmore efficient since it only stores the max-pooling indices of Eur. Conf. Comput. Vis., 2012, pp. 746–760.[25] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autono-the feature maps and uses them in its decoder network to mous driving? the KITTI vision benchmark suite,” in Proc. IEEEachieve good performance. On large andwell known datasets Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354–3361.SegNet performs competitively, achieving high scores for [26] J. Shotton, M. Johnson, and R. Cipolla, “Semantic texton forests forroad scene understanding. End-to-end learning of deep seg- image categorization and segmentation,” in Proc. IEEE Conf. Com-put. Vis. Pattern Recognit., 2008, pp. 1–8.mentation architectures is a harder challenge and we hope to [27] G. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, “Segmentationseemore attention paid to this important problem. and recognition using structure from motion point clouds,” inProc. 10th Eur. Conf. Comput. Vis., 2008, pp. 44–57.REFERENCES [28] P. Sturgess, K. Alahari, L. Ladicky, and P. H. S. Torr, “Combiningappearance and structure from motion features for road scene[1] K. Simonyan and A. Zisserman, “Very deep convolutional net- understanding,” in Proc. British Mach. Vis. Conf., 2009.works for large-scale image recognition,” arXiv:1409.1556, 2014. [29] L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr,[2] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional net- “What, where and how many? combining object detectors andworks for semantic segmentation,” in Proc. IEEE Conf. Comput. CRFs,” in Proc. 11th Eur. Conf. Comput. Vis., 2010, pp. 424–437.Vis. Pattern Recognit., 2015, pp. 3431–3440. [30] P. Kontschieder, S. R. Bulo, H. Bischof, and M. Pelillo, “Structured[3] C. Liang-Chieh, G. Papandreou, I. Kokkinos, K. Murphy, and class-labels in random forests for semantic image labelling,” inA. Yuille, “Semantic image segmentation with deep convolutional Proc. IEEE Int. Conf. Comput. Vis., 2011, pp. 2190–2197.nets and fully connected CRFs,” in Proc. Int. Conf. Learn. Represen- [31] C. Zhang, L. Wang, and R. Yang, “Semantic segmentation oftations, 2015. urban scenes using dense depth maps,” in Proc. 11th Eur. Conf.[4] H. Noh, S. Hong, and B. Han, “Learning deconvolution network Comput. Vis., 2010, pp. 708–721.for semantic segmentation,” in Proc. IEEE Int. Conf. Comput. Vis., [32] J. Tighe and S. Lazebnik, “Superparsing,” Int. J. Comput. Vis.,2015, pp. 1520–1528. vol. 101, no. 2, pp. 329–349, 2013.BADRINARAYANAN ET AL.: SEGNET: A DEEP CONVOLUTIONAL ENCODER-DECODER ARCHITECTURE FOR IMAGE SEGMENTATION 2495[33] X. Ren, L. Bo, and D. Fox, “RGB-(D) scene labeling: Features and [60] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman,algorithms,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., “LabelMe: A database and Web-based tool for image annotation,”2012, pp. 2759–2766. Int. J. Comput. Vis., vol. 77, no. 1–3, pp. 157–173, 2008.[34] A. Hermans, G. Floros, and B. Leibe, “Dense 3D semantic map- [61] M. Cordts, et al., “The cityscapes dataset for semantic urban sceneping of indoor scenes from RGB-D images,” in Proc. Int. Conf. understanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog.,Robot. Autom., 2014, pp. 2631–2638. 2016.[35] S. Gupta, P. Arbelaez, and J. Malik, “Perceptual organization and [62] V. Koltun, “Efficient inference in fully connected CRFs with gauss-recognition of indoor scenes from RGB-D images,” in Proc. IEEE ian edge potentials,” in Proc. Advances Neural Inf. Process. Syst., 2011.Conf. Comput. Vis. Pattern Recognit., 2013, pp. 564–571. [63] Bulo, S. Rota, and P. Kontschieder, “Neural decision forests for[36] C. Farabet, C. Couprie, L. Najman, and Y. LeCun, “Scene parsing semantic image labelling,” in Proc. IEEE Conf. Comput. Vis. Patternwith multiscale feature learning, purity trees, and optimal Recognit., 2014, pp. 81–88.covers,” in Proc. 29th Int. Conf. Mach. Learn., 2012, pp. 575–582. [64] Y. Yang, Z. Li, L. Zhang, C. Murphy, J. Ver Hoeve, and H. Jiang,[37] D. Grangier, L. Bottou, and R. Collobert, “Deep convolutional net- “Local label descriptor for example based semantic image label-works for scene parsing,” in Proc. ICMLWorkshopDeep Learn., 2009. ing,” in Proc. 12th Eur. Conf. Comput. Vis., 2012, pp. 361–375.[38] C. Gatta, A. Romero, and J. van de Weijer, “Unrolling loopy top- [65] Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang, “Semantic image seg-down semantic feedback in convolutional deep networks,” in mentation via deep parsing network,” in Proc. IEEE Int. Conf. Com-Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 498–505. put. Vis., 2015, pp. 1377–1385.[39] P. Pinheiro and R. Collobert, “Recurrent convolutional neural net- [66] D. Eigen and R. Fergus, “Predicting depth, surface normals andworks for scene labeling,” in Proc. 31st Int. Conf. Mach. Learn., semantic labels with a common multi-scale convolutional2014, pp. 82–90. architecture,” Proc. IEEE Int. Conf. Comput. Vis., 2015.[40] O. Russakovsky, et al., “ImageNet large scale visual recognition [67] A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, andchallenge,” Int. J. Comput. Vis., vol. 115, pp. 1–42, Apr. 2015. R. Cipolla, “SceneNet: Understanding real world indoor scenes[41] T.-Y. Lin, et al., “Microsoft COCO: Common objects in context,” in with synthetic data,” in Proc. IEEE Conf. Comput. Vis. PatternProc. 13th Eur. Conf. Comput. Vis., 2014, pp. 740–755. Recognit., 2016.[42] A. G. Schwing and R. Urtasun, “Fully connected deep structured [68] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian approxima-networks,” arXiv:1503.02351, 2015. tion: Insights and applications,” in Proc. ICML Deep Learn. Work-[43] G. Lin, et al., “Efficient piecewise training of deep structured shop, 2015.models for semantic segmentation,” arXiv:1504.01013, 2015. [69] A. Kendall, V. Badrinarayanan, and R. Cipolla, “Bayesian SegNet:[44] B.Hariharan, P. Arbelaez, R. Girshick, and J.Malik, “Hypercolumns Model uncertainty in deep convolutional encoder-decoder archi-for object segmentation and fine-grained localization,” in Proc. IEEE tectures for scene understanding,” arXiv:1511.02680, 2015.Conf. Comput. Vis. Pattern Recognit., 2015, pp. 447–456.[45] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich, “Feedfor- Vijay Badrinarayanan received the PhD degreeward semantic segmentation with zoom-out features,” in Proc. from INRIA Rennes, France, in 2009. He was aIEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 3376–3385. senior post-doctoral research associate in the[46] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, Machine Intelligence Laboratory, Department of“Deconvolutional networks,” in Proc. IEEE Conf. Comput. Vis. Pat- Engineering, University of Cambridge, Unitedtern Recognit., 2010, pp. 2528–2535. Kingdom. He currently works as a principal engi-[47] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor, M. Mathieu, neer, deep learning with Magic Leap, Inc., Moun-and Y. LeCun, “Learning convolutional feature hierarchies for tain View, California. His research interestsvisual recognition,” in Proc. Advances Neural Inf. Process. Syst., include probabilistic graphical models, deep2010, pp. 1090–1098. learning applied to image, and video based per-[48] C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convo- ception problems.lutional network for image super-resolution,” in Proc. 13th Eur.Conf. Comput. Vis., 2014, pp. 184–199. Alex Kendall received the BEng (1st class[49] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from Hons.) degree from the University of Auckland,a single image using a multi-scale deep network,” in Proc. Advan- New Zealand, in 2013. In 2014, he received theces Neural Inf. Process. Syst., 2014, pp. 2366–2374. Woolf Fisher Scholarship to study toward the[50] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating PhD degree at the University of Cambridge,deep network training by reducing internal covariate shift,” arXiv United Kingdom. He is a member of the Machinepreprint arXiv:1502.03167, 2015. Intelligence Laboratory and is interested in appli-[51] V. Badrinarayanan, B.Mishra, and R. Cipolla, “Understanding sym- cations of deep learning for mobile robotics.metries in deep networks,” arXiv preprint arXiv:1511.01029, 2015.[52] H. Noh, S. Hong, and B. Han, “Learning deconvolution networkfor semantic segmentation,” Proc. IEEE Int. Conf. Comput. Vis.,2015, pp. 1520–1528.[53] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun, “What is Roberto Cipolla received the BA degree in engi-the best multi-stage architecture for object recognition?” in Proc. neering from the University of Cambridge, inIEEE 12th Int. Conf. Comput. Vis., 2009, pp. 2146–2153. 1984, the MSE degree in electrical engineering[54] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: from the University of Pennsylvania, in 1985, andSurpassing human-level performance on ImageNet classi- the DPhil degree in computer vision from the Uni-fication,” in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1026–1034. versity of Oxford, in 1991. From 1991-92 he was[55] Y. Jia, et al., “Caffe: Convolutional architecture for fast feature a Toshiba fellow and engineer in the Toshiba Cor-embedding,” in Proc. 22nd ACM Int. Conf. Multimedia, 2014, poration Research and Development Centre,pp. 675–678. Kawasaki, Japan. He joined the Department of[56] G. Csurka, D. Larlus, F. Perronnin, and F. Meylan, “What is a Engineering, University of Cambridge, in 1992 asgood evaluation measure for semantic segmentation?” in Proc. a lecturer and a fellow of Jesus College. He24th British Mach. Vis. Conf., 2013. became a reader in information engineering in 1997 and a professor in[57] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional net- 2000. He became a fellow of the Royal Academy of Engineeringworks for semantic segmentation,” 2016. [Online]. Available: (FREng), in 2010. His research interests include computer vision andhttps://arxiv.org/pdf/1605.06211v1.pdf robotics. He has authored 3 books, edited 9 volumes and co-authored[58] D. R. Martin, C. C. Fowlkes, and J. Malik, “Learning to detect nat- more than 300 papers. He is a senior member of the IEEE.ural image boundaries using local brightness, color, and texturecues,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, no. 5, " For more information on this or any other computing topic,pp. 530–549, May 2004.please visit our Digital Library at www.computer.org/publications/dlib.[59] S. Gould, R. Fulton, and D. Koller, “Decomposing a scene intogeometric and semantically consistent regions,” in Proc. IEEE 12thInt. Conf. Comput. Vis., 2009, pp. 1–8.